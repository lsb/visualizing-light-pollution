{"ast":null,"code":"import _construct from \"/home/lsb/wikidatageo/dark-sky-vacations/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/construct\";\nimport _toConsumableArray from \"/home/lsb/wikidatageo/dark-sky-vacations/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\nimport { Data } from '../data';\nimport { Schema } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\nvar noopBuf = new Uint8Array(0);\n\nvar nullBufs = function nullBufs(bitmapLength) {\n  return [noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf];\n};\n/** @ignore */\n\n\nexport function ensureSameLengthData(schema, chunks) {\n  var batchLength = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : chunks.reduce(function (l, c) {\n    return Math.max(l, c.length);\n  }, 0);\n  var data;\n  var field;\n  var i = -1,\n      n = chunks.length;\n\n  var fields = _toConsumableArray(schema.fields);\n\n  var batchData = [];\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n\n  while (++i < n) {\n    if ((data = chunks[i]) && data.length === batchLength) {\n      batchData[i] = data;\n    } else {\n      (field = fields[i]).nullable || (fields[i] = fields[i].clone({\n        nullable: true\n      }));\n      batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n\n  return [new Schema(fields), batchLength, batchData];\n}\n/** @ignore */\n\nexport function distributeColumnsIntoRecordBatches(columns) {\n  return distributeVectorsIntoRecordBatches(new Schema(columns.map(function (_ref) {\n    var field = _ref.field;\n    return field;\n  })), columns);\n}\n/** @ignore */\n\nexport function distributeVectorsIntoRecordBatches(schema, vecs) {\n  return uniformlyDistributeChunksAcrossRecordBatches(schema, vecs.map(function (v) {\n    return v instanceof Chunked ? v.chunks.map(function (c) {\n      return c.data;\n    }) : [v.data];\n  }));\n}\n/** @ignore */\n\nfunction uniformlyDistributeChunksAcrossRecordBatches(schema, columns) {\n  var fields = _toConsumableArray(schema.fields);\n\n  var batchArgs = [];\n  var memo = {\n    numBatches: columns.reduce(function (n, c) {\n      return Math.max(n, c.length);\n    }, 0)\n  };\n  var numBatches = 0,\n      batchLength = 0;\n  var i = -1,\n      numColumns = columns.length;\n  var child,\n      childData = [];\n\n  while (memo.numBatches-- > 0) {\n    for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n      childData[i] = child = columns[i].shift();\n      batchLength = Math.min(batchLength, child ? child.length : batchLength);\n    }\n\n    if (isFinite(batchLength)) {\n      childData = distributeChildData(fields, batchLength, childData, columns, memo);\n\n      if (batchLength > 0) {\n        batchArgs[numBatches++] = [batchLength, childData.slice()];\n      }\n    }\n  }\n\n  return [schema = new Schema(fields, schema.metadata), batchArgs.map(function (xs) {\n    return _construct(RecordBatch, [schema].concat(_toConsumableArray(xs)));\n  })];\n}\n/** @ignore */\n\n\nfunction distributeChildData(fields, batchLength, childData, columns, memo) {\n  var data;\n  var field;\n  var length = 0,\n      i = -1,\n      n = columns.length;\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n\n  while (++i < n) {\n    if ((data = childData[i]) && (length = data.length) >= batchLength) {\n      if (length === batchLength) {\n        childData[i] = data;\n      } else {\n        childData[i] = data.slice(0, batchLength);\n        data = data.slice(batchLength, length - batchLength);\n        memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n      }\n    } else {\n      (field = fields[i]).nullable || (fields[i] = field.clone({\n        nullable: true\n      }));\n      childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n\n  return childData;\n}","map":{"version":3,"sources":["util/recordbatch.ts"],"names":[],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA,SAAS,IAAT,QAA8B,SAA9B;AACA,SAAS,MAAT,QAA8B,WAA9B;AACA,SAAS,OAAT,QAAwB,mBAAxB;AACA,SAAS,WAAT,QAA4B,gBAA5B;AAEA,IAAM,OAAO,GAAG,IAAI,UAAJ,CAAe,CAAf,CAAhB;;AACA,IAAM,QAAQ,GAAG,SAAX,QAAW,CAAC,YAAD;AAAA,SAAoC,CACjD,OADiD,EACxC,OADwC,EAC/B,IAAI,UAAJ,CAAe,YAAf,CAD+B,EACD,OADC,CAApC;AAAA,CAAjB;AAIA;;;AACA,OAAM,SAAU,oBAAV,CACF,MADE,EAEF,MAFE,EAG6D;AAAA,MAA/D,WAA+D,uEAAjD,MAAM,CAAC,MAAP,CAAc,UAAC,CAAD,EAAI,CAAJ;AAAA,WAAU,IAAI,CAAC,GAAL,CAAS,CAAT,EAAY,CAAC,CAAC,MAAd,CAAV;AAAA,GAAd,EAA+C,CAA/C,CAAiD;AAE/D,MAAI,IAAJ;AACA,MAAI,KAAJ;AACA,MAAI,CAAC,GAAG,CAAC,CAAT;AAAA,MAAY,CAAC,GAAG,MAAM,CAAC,MAAvB;;AACA,MAAM,MAAM,sBAAO,MAAM,CAAC,MAAd,CAAZ;;AACA,MAAM,SAAS,GAAG,EAAlB;AACA,MAAM,YAAY,GAAG,CAAE,WAAW,GAAG,EAAf,GAAqB,CAAC,EAAvB,KAA8B,CAAnD;;AACA,SAAO,EAAE,CAAF,GAAM,CAAb,EAAgB;AACZ,QAAI,CAAC,IAAI,GAAG,MAAM,CAAC,CAAD,CAAd,KAAsB,IAAI,CAAC,MAAL,KAAgB,WAA1C,EAAuD;AACnD,MAAA,SAAS,CAAC,CAAD,CAAT,GAAe,IAAf;AACH,KAFD,MAEO;AACH,OAAC,KAAK,GAAG,MAAM,CAAC,CAAD,CAAf,EAAoB,QAApB,KAAiC,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAN,CAAU,KAAV,CAAgB;AAAE,QAAA,QAAQ,EAAE;AAAZ,OAAhB,CAA7C;AACA,MAAA,SAAS,CAAC,CAAD,CAAT,GAAe,IAAI,GAAG,IAAI,CAAC,kCAAL,CAAwC,WAAxC,CAAH,GACb,IAAI,CAAC,GAAL,CAAS,KAAK,CAAC,IAAf,EAAqB,CAArB,EAAwB,WAAxB,EAAqC,WAArC,EAAkD,QAAQ,CAAC,YAAD,CAA1D,CADN;AAEH;AACJ;;AACD,SAAO,CAAC,IAAI,MAAJ,CAAc,MAAd,CAAD,EAAwB,WAAxB,EAAqC,SAArC,CAAP;AACH;AAED;;AACA,OAAM,SAAU,kCAAV,CAA0F,OAA1F,EAAuH;AACzH,SAAO,kCAAkC,CAAI,IAAI,MAAJ,CAAc,OAAO,CAAC,GAAR,CAAY;AAAA,QAAG,KAAH,QAAG,KAAH;AAAA,WAAe,KAAf;AAAA,GAAZ,CAAd,CAAJ,EAAsD,OAAtD,CAAzC;AACH;AAED;;AACA,OAAM,SAAU,kCAAV,CAA0F,MAA1F,EAA6G,IAA7G,EAA+J;AACjK,SAAO,4CAA4C,CAAI,MAAJ,EAAY,IAAI,CAAC,GAAL,CAAS,UAAC,CAAD;AAAA,WAAO,CAAC,YAAY,OAAb,GAAuB,CAAC,CAAC,MAAF,CAAS,GAAT,CAAa,UAAC,CAAD;AAAA,aAAO,CAAC,CAAC,IAAT;AAAA,KAAb,CAAvB,GAAqD,CAAC,CAAC,CAAC,IAAH,CAA5D;AAAA,GAAT,CAAZ,CAAnD;AACH;AAED;;AACA,SAAS,4CAAT,CAAmG,MAAnG,EAAsH,OAAtH,EAAmJ;AAE/I,MAAM,MAAM,sBAAO,MAAM,CAAC,MAAd,CAAZ;;AACA,MAAM,SAAS,GAAG,EAAlB;AACA,MAAM,IAAI,GAAG;AAAE,IAAA,UAAU,EAAE,OAAO,CAAC,MAAR,CAAe,UAAC,CAAD,EAAI,CAAJ;AAAA,aAAU,IAAI,CAAC,GAAL,CAAS,CAAT,EAAY,CAAC,CAAC,MAAd,CAAV;AAAA,KAAf,EAAgD,CAAhD;AAAd,GAAb;AAEA,MAAI,UAAU,GAAG,CAAjB;AAAA,MAAoB,WAAW,GAAG,CAAlC;AACA,MAAI,CAAC,GAAW,CAAC,CAAjB;AAAA,MAAoB,UAAU,GAAG,OAAO,CAAC,MAAzC;AACA,MAAI,KAAJ;AAAA,MAA6B,SAAS,GAAuB,EAA7D;;AAEA,SAAO,IAAI,CAAC,UAAL,KAAoB,CAA3B,EAA8B;AAE1B,SAAK,WAAW,GAAG,MAAM,CAAC,iBAArB,EAAwC,CAAC,GAAG,CAAC,CAAlD,EAAqD,EAAE,CAAF,GAAM,UAA3D,GAAwE;AACpE,MAAA,SAAS,CAAC,CAAD,CAAT,GAAe,KAAK,GAAG,OAAO,CAAC,CAAD,CAAP,CAAW,KAAX,EAAvB;AACA,MAAA,WAAW,GAAG,IAAI,CAAC,GAAL,CAAS,WAAT,EAAsB,KAAK,GAAG,KAAK,CAAC,MAAT,GAAkB,WAA7C,CAAd;AACH;;AAED,QAAI,QAAQ,CAAC,WAAD,CAAZ,EAA2B;AACvB,MAAA,SAAS,GAAG,mBAAmB,CAAC,MAAD,EAAS,WAAT,EAAsB,SAAtB,EAAiC,OAAjC,EAA0C,IAA1C,CAA/B;;AACA,UAAI,WAAW,GAAG,CAAlB,EAAqB;AACjB,QAAA,SAAS,CAAC,UAAU,EAAX,CAAT,GAA0B,CAAC,WAAD,EAAc,SAAS,CAAC,KAAV,EAAd,CAA1B;AACH;AACJ;AACJ;;AACD,SAAO,CACH,MAAM,GAAG,IAAI,MAAJ,CAAc,MAAd,EAAsB,MAAM,CAAC,QAA7B,CADN,EAEH,SAAS,CAAC,GAAV,CAAc,UAAC,EAAD;AAAA,sBAAY,WAAZ,GAAwB,MAAxB,4BAAmC,EAAnC;AAAA,GAAd,CAFG,CAAP;AAIH;AAED;;;AACA,SAAS,mBAAT,CAA0E,MAA1E,EAAuG,WAAvG,EAA4H,SAA5H,EAA2J,OAA3J,EAA0L,IAA1L,EAAsN;AAClN,MAAI,IAAJ;AACA,MAAI,KAAJ;AACA,MAAI,MAAM,GAAG,CAAb;AAAA,MAAgB,CAAC,GAAG,CAAC,CAArB;AAAA,MAAwB,CAAC,GAAG,OAAO,CAAC,MAApC;AACA,MAAM,YAAY,GAAG,CAAE,WAAW,GAAG,EAAf,GAAqB,CAAC,EAAvB,KAA8B,CAAnD;;AACA,SAAO,EAAE,CAAF,GAAM,CAAb,EAAgB;AACZ,QAAI,CAAC,IAAI,GAAG,SAAS,CAAC,CAAD,CAAjB,KAA0B,CAAC,MAAM,GAAG,IAAI,CAAC,MAAf,KAA0B,WAAxD,EAAsE;AAClE,UAAI,MAAM,KAAK,WAAf,EAA4B;AACxB,QAAA,SAAS,CAAC,CAAD,CAAT,GAAe,IAAf;AACH,OAFD,MAEO;AACH,QAAA,SAAS,CAAC,CAAD,CAAT,GAAe,IAAI,CAAC,KAAL,CAAW,CAAX,EAAc,WAAd,CAAf;AACA,QAAA,IAAI,GAAG,IAAI,CAAC,KAAL,CAAW,WAAX,EAAwB,MAAM,GAAG,WAAjC,CAAP;AACA,QAAA,IAAI,CAAC,UAAL,GAAkB,IAAI,CAAC,GAAL,CAAS,IAAI,CAAC,UAAd,EAA0B,OAAO,CAAC,CAAD,CAAP,CAAW,OAAX,CAAmB,IAAnB,CAA1B,CAAlB;AACH;AACJ,KARD,MAQO;AACH,OAAC,KAAK,GAAG,MAAM,CAAC,CAAD,CAAf,EAAoB,QAApB,KAAiC,MAAM,CAAC,CAAD,CAAN,GAAY,KAAK,CAAC,KAAN,CAAY;AAAE,QAAA,QAAQ,EAAE;AAAZ,OAAZ,CAA7C;AACA,MAAA,SAAS,CAAC,CAAD,CAAT,GAAe,IAAI,GAAG,IAAI,CAAC,kCAAL,CAAwC,WAAxC,CAAH,GACb,IAAI,CAAC,GAAL,CAAS,KAAK,CAAC,IAAf,EAAqB,CAArB,EAAwB,WAAxB,EAAqC,WAArC,EAAkD,QAAQ,CAAC,YAAD,CAA1D,CADN;AAEH;AACJ;;AACD,SAAO,SAAP;AACH","sourcesContent":["// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\nimport { Column } from '../column';\nimport { Vector } from '../vector';\nimport { DataType } from '../type';\nimport { Data, Buffers } from '../data';\nimport { Schema, Field } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\n\nconst noopBuf = new Uint8Array(0);\nconst nullBufs = (bitmapLength: number) => <unknown> [\n    noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf\n] as Buffers<any>;\n\n/** @ignore */\nexport function ensureSameLengthData<T extends { [key: string]: DataType } = any>(\n    schema: Schema<T>,\n    chunks: Data<T[keyof T]>[],\n    batchLength = chunks.reduce((l, c) => Math.max(l, c.length), 0)\n) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let i = -1, n = chunks.length;\n    const fields = [...schema.fields];\n    const batchData = [] as Data<T[keyof T]>[];\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = chunks[i]) && data.length === batchLength) {\n            batchData[i] = data;\n        } else {\n            (field = fields[i]).nullable || (fields[i] = fields[i].clone({ nullable: true }) as Field<T[keyof T]>);\n            batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return [new Schema<T>(fields), batchLength, batchData] as [Schema<T>, number, Data<T[keyof T]>[]];\n}\n\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches<T extends { [key: string]: DataType } = any>(columns: Column<T[keyof T]>[]): [Schema<T>, RecordBatch<T>[]] {\n    return distributeVectorsIntoRecordBatches<T>(new Schema<T>(columns.map(({ field }) => field)), columns);\n}\n\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, vecs: (Vector<T[keyof T]> | Chunked<T[keyof T]>)[]): [Schema<T>, RecordBatch<T>[]] {\n    return uniformlyDistributeChunksAcrossRecordBatches<T>(schema, vecs.map((v) => v instanceof Chunked ? v.chunks.map((c) => c.data) : [v.data]));\n}\n\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, columns: Data<T[keyof T]>[][]): [Schema<T>, RecordBatch<T>[]] {\n\n    const fields = [...schema.fields];\n    const batchArgs = [] as [number, Data<T[keyof T]>[]][];\n    const memo = { numBatches: columns.reduce((n, c) => Math.max(n, c.length), 0) };\n\n    let numBatches = 0, batchLength = 0;\n    let i: number = -1, numColumns = columns.length;\n    let child: Data<T[keyof T]>, childData: Data<T[keyof T]>[] = [];\n\n    while (memo.numBatches-- > 0) {\n\n        for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n            childData[i] = child = columns[i].shift()!;\n            batchLength = Math.min(batchLength, child ? child.length : batchLength);\n        }\n\n        if (isFinite(batchLength)) {\n            childData = distributeChildData(fields, batchLength, childData, columns, memo);\n            if (batchLength > 0) {\n                batchArgs[numBatches++] = [batchLength, childData.slice()];\n            }\n        }\n    }\n    return [\n        schema = new Schema<T>(fields, schema.metadata),\n        batchArgs.map((xs) => new RecordBatch(schema, ...xs))\n    ];\n}\n\n/** @ignore */\nfunction distributeChildData<T extends { [key: string]: DataType } = any>(fields: Field<T[keyof T]>[], batchLength: number, childData: Data<T[keyof T]>[], columns: Data<T[keyof T]>[][], memo: { numBatches: number }) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let length = 0, i = -1, n = columns.length;\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = childData[i]) && ((length = data.length) >= batchLength)) {\n            if (length === batchLength) {\n                childData[i] = data;\n            } else {\n                childData[i] = data.slice(0, batchLength);\n                data = data.slice(batchLength, length - batchLength);\n                memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n            }\n        } else {\n            (field = fields[i]).nullable || (fields[i] = field.clone({ nullable: true }) as Field<T[keyof T]>);\n            childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return childData;\n}\n"]},"metadata":{},"sourceType":"module"}